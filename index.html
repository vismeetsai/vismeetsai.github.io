<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
	<link rel="icon" href="data:,">
	<link type="text/css" media="all" href="./css/style.css" rel="stylesheet">
	<title>Vis Meets AI</title>
	<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1.0, maximum-scale=1.0">
</head>

<body data-rsssl="1" class="projects-template-default single single-projects postid-49496 vsc-initialized">
	<header id="page-top">
		<div class="cols wrapper2 clearfix">
			<div class="col col2 top-container">
				<img src="./image/logo_red.png" class="logo">
			</div>
			<div class="col col2">
				<br>
				<h1>Visualization Meets AI 2024</h1>
			</div>
		</div>
	</header>
	<div class="page-outer">
		<br>
		<div class="project-entry">
			<div class="content-block page-entry default">
				<div class="cols wrapper2 clearfix">
					<div class="col col2">
						<h2>Goals</h2>
					</div>
					<div class="col col2">
						<p class="mb0">
							The task of data visualization generally involves a design step, which requires the
							knowledge of the data domain and visualization methods to do well. Because of the immense
							design space for optimization, it can take both novices and
							experts substantial effort to derive desired visualization results from data for exploration
							or communication. Following the resurgence of artificial intelligence (AI) technology in
							recent years, in the field of visualization, there is
							a growing interest and opportunity in applying AI to perform data transformation and to
							assist the generation of visualization, aiming to strike a balance between cost and quality.
							The use of visualization to enhance AI is the other
							active line of research. This workshop, held in conjunction with <a
								href="https://pacificvis.github.io/pvis2024/">IEEE&nbsp;PacificVis&nbsp;2024</a>, aims
							at exploring this emerging area of research and practice by fostering communication
							between visualization researchers and practitioners. Attendees will be introduced to the
							latest and greatest research innovations in AI-enhanced visualization (AI4VIS) as well as
							visualization-enhanced AI (VIS4AI), and also learn about further research opportunities. The
							workshop will be composed of full-paper presentations, short-paper presentations, and
							invited talks.
						</p>
					</div>
				</div>
			</div>
		</div>
		<div class="content-block page-entry gray-bgr">
			<div class="cols wrapper2 clearfix">
				<div class="col col2">
					<h2>Call for Participation</h2>
				</div>
				<div class="col2 col">
					<h3>Submission</h3>
					<p>
						We welcome contributions as full papers and short papers.
						All accepted papers will appear in the proceedings of the IEEE PacificVis 2024 and the IEEE
						Xplore Digital Library.
					</p>
					<p>
						Papers should follow the <a
							href="https://tc.computer.org/vgtc/publications/conference/">formatting guidelines for VGTC
							Conference Style Papers</a>.
						There is no strict page limit, but authors are encouraged to submit a paper whose length matches
						its contribution.
						Our recommendation for the paper length is as follows:
					</p>
					<p> - Full paper: up to 10 pages (including reference)</p>
					<p> - Short paper: up to 6 pages (including reference)</p>
					<h4></h4>
					<p>
						Both full papers and short papers are to be submitted using <a
							href="https://new.precisionconference.com/vgtc">PCS</a> (Track Name: PacificVis 2024
						Visualization Meets AI Workshop). We will accept both single-blind (not anonymized) as well as
						double-blind (anonymized) submissions. In the case of double-blind submissions, please
						substitute the author names with the paper ID number.
					</p>

					<h3>Important Dates (for both full papers and short papers)</h3>
					<p>- Paper due: December 29, 2023</p>
					<p>- 1st cycle notification: February 2, 2024</p>
					<p>- Revision due: February 16, 2024</p>
					<p>- 2nd cycle notification: February 26, 2024</p>
					<p>- Camera ready paper due: March 4, 2024</p>
					<p>- Workshop: April 23, 2024</p>
					<p>All deadlines are due at 11:59pm (23:59) Anywhere on Earth (AoE).</p>
					<br>

					<h3>Topics of Interest</h3>
					<p>
						We encourage submissions of high quality research and application papers incorporating
						visualization and AI/machine lerning. Our interest includes both topics of AI4VIS and VIS4AI.
					<p>Example papers:</p>
					<div class="paper-cate">AI4VIS</div>
					<div class="papers">J. Han and C. Wang. "<a
							href="https://www.sciencedirect.com/science/article/pii/S2468502X22000213">VCNet: A
							generative model for volume completion</a>." Visual Informatics, 6(2): 2022.</div>
					<div class="papers">L. Giovannangeli, R. Bourqui, R. Giot, and D. Auber. "<a
							href="https://www.sciencedirect.com/science/article/pii/S2468502X20300140">Toward automatic
							comparison of visualization techniques: Application to graph
							visualization</a>." Visual Informatics, 4(2): 2020.</div>
					<!-- <div class="papers">R. Guo, T. Fujiwara, Y. Li, K. M. Lima, S. Sen, N. K. Tran, and K.-L. Ma. "<a href="https://www.sciencedirect.com/science/article/pii/S2468502X20300139">Comparative visual analytics for assessing medical records with
							sequence embedding</a>." Visual Informatics, 4(2): 2020.</div> -->
					<!-- <div class="papers">W. He, J. Wang, H. Guo, H.-W. Shen, and T. Peterka. "CECAV-DNN: Collective ensemble comparison and visualization using deep neural networks." Visual Informatics, 4(2): 2020.</div> -->
					<div class="papers">J. Shen, R. Wang, and H.-W. Shen. "<a
							href="https://www.sciencedirect.com/science/article/pii/S2468502X20300152">Visual
							exploration of latent space for traditional Chinese music</a>." Visual Informatics, 4(2):
						2020.
					</div>

					<div class="paper-cate">VIS4AI</div>
					<!-- <div class="papers">P. Chawla, S. Hazarika, and H.-W. Shen. "Token-wise sentiment decomposition for ConvNet: Visualizing a sentiment classifier." Visual Informatics, 4(2): 2020.</div> -->
					<div class="papers">M. Gleicher, X. Yu, and Y. Chen. "<a
							href="https://www.sciencedirect.com/science/article/pii/S2468502X22000195">Trinary tools for
							continuously valued binary classifiers</a>." Visual Informatics, 6(2): 2022.</div>
					<div class="papers">X. Ji, Y. Tu, W. He, J. Wang, H.-W. Shen, and P.-Y. Yen. "<a
							href="https://www.sciencedirect.com/science/article/pii/S2468502X21000097">USEVis: Visual
							analytics of attention-based neural embedding in information
							retrieval</a>." Visual Informatics, 5(2): 2021.</div>
					<!-- <div class="papers">Y. Li, T. Fujiwara, Y. K. Choi, K. K. Kim, and K.-L. Ma. "<a href="https://www.sciencedirect.com/science/article/pii/S2468502X20300176">A visual analytics system for multi-model comparison on clinical data
							predictions</a>." Visual Informatics, 4(2): 2020.</div> -->
					<div class="papers">M. Wang, J. Wenskovitch, L. House, N. Polys, and C. North. "<a
							href="https://www.sciencedirect.com/science/article/pii/S2468502X21000085">Bridging
							cognitive gaps between user and model in interactive dimension
							reduction</a>." Visual Informatics, 5(2): 2021.</div>

					<br>
				</div>
				<br>
			</div>
		</div>
		<div class="content-block page-entry default ">
			<div class="cols wrapper2 clearfix">
				<div class="col2 col">
					<h2>People</h2>
				</div>
				<div class="col2 col">
					<h3>Workshop Chair</h3>
					<p class="people">Takanori Fujiwara, Link√∂ping University</p>
					<p class="people">Junpeng Wang, Visa Research</p>
					<br>
					<h3>Program Committee</h3>
					<p class="people">Angelos Chatzimparmpas, Northwestern University</p>
					<p class="people">Dylan Cashman, Brandeis University</p>
					<p class="people">Steffen Frey, University of Groningen</p>
					<p class="people">Hanqi Guo, the Ohio State University</p>
					<p class="people">Jun Han, the Chinese University of Hong Kong</p>
					<p class="people">Wenbin He, Bosch Research</p>
					<p class="people">Oh-Hyun Kwon, Apple</p>
					<p class="people">Mengchen Liu, Microsoft Research</p>
					<p class="people">Shusen Liu, Lawrence Livermore National Laboratory</p>
					<p class="people">Huan Song, Amazon AWS AI</p>
					<p class="people">Jun Tao, Sun Yat-sen University</p>
					<p class="people">Qianwen Wang, University of Minnesota</p>
					<p class="people">Yong Wang, Singapore Management University</p>
					<p class="people">John Wenskovitch, Pacific Northwest National Laboratory</p>
					<p class="people">Jiazhi Xia, Central South University</p>
					<p class="people">Jun Yuan, Apple</p>
					<br>
				</div>
			</div>
		</div>
		<div class="content-block page-entry default no-padding-top gray-bgr" style="padding-top:0px;">
			<div class="cols wrapper2 clearfix">
				<div class="col2 col">
					<h2>Past Events</h2>
				</div>
				<div class="col2 col">
					<p><a href="./2023/index.html">Visualization Meets AI 2023</a></p>
					<p><a href="./2022/index.html">Visualization Meets AI 2022</a></p>
					<p><a href="./2021/index.html">Visualization Meets AI 2021</a></p>
					<p><a href="./2020/index.html">Visualization Meets AI 2020</a></p>
				</div>
				<br />
			</div>
		</div>
		<div class="content-block page-entry dark">
			<div class="cols wrapper2 clearfix">
				<div class="col2 col footer-div">
					<p>&nbsp;&nbsp;</p>
				</div>
				<div class="col2 col">
					<br />
					<div class="the-block">
						<h3 class="selectionShareable">CONTACT</h3>
						<p>pvis_ai4vis@pvis.org</p>
						<br />
					</div>
				</div>
				<br>
			</div>
		</div>
	</div>
</body>

</html>
<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
	<title>Vis Meets AI</title>
	<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&family=Poppins:wght@600&display=swap"
		rel="stylesheet">
	<link type="text/css" media="all" href="./css/style.css" rel="stylesheet">
</head>

<body>
	<section class="hero" id="header">
		<img src="./image/logo_red.png" alt="VisMeetAI Logo" class="logo" />
		<h1>Visualization Meets AI 2026</h1>
		<p>April, 2026 • Sydney, Australia</p>
		<div class="buttons">
			<a class="button" href="#cfp">Call for Participation</a>
			<a class="button" href="#dates">Important Dates</a>
			<a class="button" href="#committees">Committees</a>
			<a class="button" href="#past">Past Events</a>
		</div>
	</section>

	<section class="section" id="goals">
		<h2>Goals</h2>
		<p>
			Data visualization requires a thoughtful design process that heavily relies on both domain knowledge
			and familiarity with visualization techniques. Given the vast design space and inherent complexity, even
			experts often invest substantial effort to create effective visualizations for exploration or communication.
		</p>
		<p>
			With the rapid advancement of artificial intelligence (AI)&mdash;particularly the rise of powerful
			foundation models such as large language models (LLMs), vision-language models (VLMs), and multimodal AI
			systems&mdash;the field of visualization is undergoing a significant transformation. These cutting-edge
			models present new opportunities to automate and augment the visualization process. For instance, LLMs can
			translate natural language queries into visual specifications, assist with data wrangling, and recommend
			appropriate visualization types. VLMs and multimodal models enable deeper data understanding and interaction
			by integrating textual, visual, and tabular information, further advancing the creation of intuitive and
			intelligent visualizations.
		<p>
			Concurrently, visualization plays an increasingly vital role in the development and deployment of advanced
			AI models. As these models grow in complexity and scale, the need for effective visual interfaces and
			techniques becomes more pressing—to interpret model behavior, debug outputs, and foster transparency,
			accountability, and human-AI collaboration.
		</p>
		<p>
			This workshop, held in conjunction with
			<a href="https://pacificvis2026.github.io/">IEEE&nbsp;PacificVis&nbsp;2026</a>,
			aims to explore this dynamic and rapidly evolving area by fostering communication between the visualization
			and AI communities. Attendees will engage with the latest research at the intersection of AI-enhanced
			visualization (AI4VIS) and visualization-enhanced AI (VIS4AI), with a particular focus on how cutting-edge
			models&mdash;such as LLMs, VLMs, and beyond&mdash;are reshaping the landscape.
		</p>
	</section>

	<section class="section" id="cfp">
		<h2>Call for Participation</h2>
		<h3>Submission</h3>
		<p>
			We welcome submissions in the form of full papers. All accepted papers will be published in a special issue
			of the
			<i><a href="https://journals.sagepub.com/home/IVI">Information Visualization</a></i> journal.
		</p>
		<p>
			Authors should follow the instructions under "Preparing your manuscript for submission" as outlined in the
			journal's
			<a href="https://journals.sagepub.com/author-instructions/IVI">submission guidelines</a>.
			A LaTeX template is available under the "Article format" section at
			<a href="https://www.sagepub.com/journals/information-for-authors/preparing-your-manuscript">this link</a>.
			While there is no strict page limit, we encourage authors to ensure that the length of the paper is
			appropriate for the scope and significance of the contribution.
		</p>
		<p>
			Submissions must be made through <a href="https://new.precisionconference.com/vgtc">PCS</a> (Track Name:
			PacificVis 2026 Visualization Meets AI Workshop).
			Only double-blind (anonymized) submissions will be accepted. Please replace author names with the paper ID
			number to ensure anonymity.
		</p>

		<h3 id="dates">Important Dates (Tentative)</h3>
		<div class="timeline">
			<div class="timeline-item">
				<div class="timeline-date">December 22, 2025: Paper due</div>
				<div class="timeline-desc"></div>
			</div>
			<div class="timeline-item">
				<div class="timeline-date">January 30, 2026: 1st cycle notification from workshop chairs</div>
				<div class="timeline-desc">
					Conditionally accepted papers need to go through minor revisions and to be reviewed in the second
					review cycle.
				</div>
			</div>
			<div class="timeline-item">
				<div class="timeline-date">February 13, 2026: Revision due</div>
				<div class="timeline-desc"></div>
			</div>
			<div class="timeline-item">
				<div class="timeline-date">February 23, 2026: 2nd cycle notification from workshop chairs</div>
				<div class="timeline-desc">
					Workshop chairs will recommend acceptance to the Editor-in-Chief (EIC) of <i>Information Visualization</i>
					if the revisions are deemed satisfactory. However, papers with inadequate revisions may still be
					rejected in this cycle.
				</div>
			</div>
			<div class="timeline-item">
				<div class="timeline-date">March 2, 2026: Editable source files due</div>
				<div class="timeline-desc"></div>
			</div>
			<div class="timeline-item">
				<div class="timeline-date">March 9, 2026: Final notification from the EIC of
					<i>Information Visualization</i></div>
				<div class="timeline-desc"></div>
			</div>
			<div class="timeline-item">
				<div class="timeline-date">April 20, 2026: Workshop</div>
			</div>
			<div></div>
		</div>
		<h3>Topics of Interest</h3>
		<p>
			We invite high-quality research and application papers that integrate visualization and AI/machine learning.
			Submissions may address topics in both AI for Visualization (AI4VIS) and Visualization for AI (VIS4AI).
		</p>
		<p>Example papers:</p>
		<div class="paper-cate">AI4VIS</div>
		<div class="papers">P.-P. V&aacute;zquez.
			<a href="https://doi.org/10.1109/PacificVis60374.2024.00049">
				Are LLMs ready for Visualization?</a>
			In <i>2024 IEEE 17th Pacific Visualization Conference (PacificVis)</i>, pp. 343-352, 2024.
		</div>
		<div class="papers">J. Han and C. Wang.
			<a href="https://www.sciencedirect.com/science/article/pii/S2468502X22000213">
				VCNet: A Generative Model for Volume Completion.</a>
			<i>Visual Informatics</i>, 6(2): 62-73, 2022.
		</div>
		<div class="papers">L. Giovannangeli, R. Bourqui, R. Giot, and D. Auber.
			<a href="https://www.sciencedirect.com/science/article/pii/S2468502X20300140">
				Toward Automatic Comparison of Visualization Techniques: Application to Graph
				Visualization.</a>
			<i>Visual Informatics</i>, 4(2): 86-98, 2020.
		</div>
		<!-- <div class="papers">R. Guo, T. Fujiwara, Y. Li, K. M. Lima, S. Sen, N. K. Tran, and K.-L. Ma. "<a href="https://www.sciencedirect.com/science/article/pii/S2468502X20300139">Comparative visual analytics for assessing medical records with
							sequence embedding</a>." Visual Informatics, 4(2): 2020.</div> -->
		<!-- <div class="papers">W. He, J. Wang, H. Guo, H.-W. Shen, and T. Peterka. "CECAV-DNN: Collective ensemble comparison and visualization using deep neural networks." Visual Informatics, 4(2): 2020.</div> -->
		<div class="papers">J. Shen, R. Wang, and H.-W. Shen.
			<a href="https://www.sciencedirect.com/science/article/pii/S2468502X20300152">
				Visual Exploration of Latent Space for Traditional Chinese Music.</a>
			<i>Visual Informatics</i>, 4(2): 99-108, 2020.
		</div>

		<div class="paper-cate">VIS4AI</div>
		<div class="papers">Z. Liang, G. Li, R. Gu, Y. Wang, and G. Shan.
			<a href="https://doi.org/10.1109/PacificVis60374.2024.00051">
				SampleViz: Concept based Sampling for Policy Refinement in Deep Reinforcement Learning.</a>
			In <i>2024 IEEE 17th Pacific Visualization Conference (PacificVis)</i>, pp. 359-368, 2024.
		</div>
		<!-- <div class="papers">P. Chawla, S. Hazarika, and H.-W. Shen. "Token-wise sentiment decomposition for ConvNet: Visualizing a sentiment classifier." Visual Informatics, 4(2): 2020.</div> -->
		<div class="papers">M. Gleicher, X. Yu, and Y. Chen.
			<a href="https://www.sciencedirect.com/science/article/pii/S2468502X22000195">Trinary Tools for
				Continuously Valued Binary Classifiers.</a>
			<i>Visual Informatics</i>, 6(2): 74-86, 2022.
		</div>
		<div class="papers">X. Ji, Y. Tu, W. He, J. Wang, H.-W. Shen, and P.-Y. Yen.
			<a href="https://www.sciencedirect.com/science/article/pii/S2468502X21000097">
				USEVis: Visual Analytics of Attention-Based Neural Embedding in Information Retrieval.</a>
			<i>Visual Informatics</i>, 5(2): 1-12, 2021.
		</div>
		<!-- <div class="papers">Y. Li, T. Fujiwara, Y. K. Choi, K. K. Kim, and K.-L. Ma. "<a href="https://www.sciencedirect.com/science/article/pii/S2468502X20300176">A visual analytics system for multi-model comparison on clinical data
							predictions</a>." Visual Informatics, 4(2): 2020.</div> -->
		<div class="papers">M. Wang, J. Wenskovitch, L. House, N. Polys, and C. North.
			<a href="https://www.sciencedirect.com/science/article/pii/S2468502X21000085">
				Bridging Cognitive Gaps between User and Model in Interactive Dimension Reduction.</a>
			<i>Visual Informatics</i>, 5(2): 13-25, 2021.
		</div>
	</section>

	<section class="section" id="committees">
		<h2>Committees</h2>
		<h3>Workshop Chair</h3>
		<div class="committee-grid">
			<div class="photo-container">
				<img src="image/photo_takanori_fujiwara.jpg" alt="Takanori Fujiwara" class="photo">
				<!-- <p class="people">Takanori Fujiwara, University of Arizona</p> -->
				<div class="chair-info">
					<p class="chair-name">Takanori Fujiwara</p>
					<p class="chair-affiliation">University of Arizona</p>
				</div>
			</div>

			<div class="photo-container">
				<img src="image/photo_junpeng_wang.jpg" alt="Junpeng Wang" class="photo">
				<!-- <p class="people">Junpeng Wang, Visa Research</p> -->
				<div class="chair-info">
					<p class="chair-name">Junpeng Wang</p>
					<p class="chair-affiliation">Visa Research</p>
				</div>
			</div>
		</div>
		<h3>Program Committee</h3>
		<p class="people">Coming soon.</p>
		<br>
	</section>

	<section class="section" id="past">
		<h2>Past Events</h2>
		<p><a href="../2025/index.html">Visualization Meets AI 2025</a></p>
		<p><a href="../2024/index.html">Visualization Meets AI 2024</a></p>
		<p><a href="../2023/index.html">Visualization Meets AI 2023</a></p>
		<p><a href="../2022/index.html">Visualization Meets AI 2022</a></p>
		<p><a href="../2021/index.html">Visualization Meets AI 2021</a></p>
		<p><a href="../2020/index.html">Visualization Meets AI 2020</a></p>
	</section>

	<section class="section" id="contact">
		<h2>Contact</h2>
		<p>pvis_ai4vis@pvis.org</p>
	</section>

	<section class="hero" id="footer"></section>
</body>

</html>